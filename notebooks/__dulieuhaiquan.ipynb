{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a576d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub GopDuLieuVaoCacSheet()\n",
    "#     Dim ThuMuc As String\n",
    "#     Dim TenFile As String\n",
    "#     Dim WbNguon As Workbook\n",
    "#     Dim WbTongHop As Workbook\n",
    "#     Dim wsNguon As Worksheet\n",
    "#     Dim wsMoi As Worksheet\n",
    "\n",
    "#     ' Ch·ªçn th∆∞ m·ª•c ch·ª©a c√°c file\n",
    "#     With Application.FileDialog(msoFileDialogFolderPicker)\n",
    "#         .Title = \"Ch·ªçn th∆∞ m·ª•c ch·ª©a c√°c file Excel\"\n",
    "#         If .Show <> -1 Then Exit Sub\n",
    "#         ThuMuc = .SelectedItems(1) & \"\\\"\n",
    "#     End With\n",
    "\n",
    "#     Application.ScreenUpdating = False\n",
    "#     Application.DisplayAlerts = False\n",
    "\n",
    "#     Set WbTongHop = ThisWorkbook\n",
    "#     TenFile = Dir(ThuMuc & \"*.xls*\")\n",
    "\n",
    "#     Do While TenFile <> \"\"\n",
    "#         If ThuMuc & TenFile <> WbTongHop.FullName Then\n",
    "#             Set WbNguon = Workbooks.Open(ThuMuc & TenFile)\n",
    "#             Set wsNguon = WbNguon.Sheets(1)\n",
    "\n",
    "#             ' T·∫°o sheet m·ªõi v√† copy d·ªØ li·ªáu\n",
    "#             Set wsMoi = WbTongHop.Sheets.Add(After:=WbTongHop.Sheets(WbTongHop.Sheets.Count))\n",
    "#             wsMoi.Name = Left(TenFile, InStrRev(TenFile, \".\") - 1)\n",
    "#             wsNguon.UsedRange.Copy Destination:=wsMoi.Range(\"A1\")\n",
    "\n",
    "#             WbNguon.Close SaveChanges:=False\n",
    "#         End If\n",
    "#         TenFile = Dir\n",
    "#     Loop\n",
    "\n",
    "#     Application.ScreenUpdating = True\n",
    "#     Application.DisplayAlerts = True\n",
    "\n",
    "#     MsgBox \"ƒê√£ g·ªôp xong t·∫•t c·∫£ d·ªØ li·ªáu v√†o c√°c sheet!\"\n",
    "# End Sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "462ab68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä K·∫øt qu·∫£ ph√¢n lo·∫°i sheet theo ti√™u ƒë·ªÅ c·ªôt:\n",
      "\n",
      "üîπ Nh√≥m 1 - Header: []\n",
      "   ‚û§ Sheet: Sheet1\n",
      "   ‚û§ Sheet: 39139090\n",
      "\n",
      "üîπ Nh√≥m 2 - Header: ['T·ªù khai', 'Ng√†y ƒëƒÉng k√Ω', 'T√™n n∆°i m·ªü t·ªù khai', 'M√£ doanh nghi·ªáp XNK', 'T√™n doanh nghi·ªáp XNK', 'ƒê∆°n v·ªã ƒë·ªëi t√°c', 'M√£ h√†ng khai b√°o', 'S·ªë th·ª© t·ª± h√†ng', 'T√™n h√†ng', 'ƒê∆°n gi√° khai b√°o(USD)', 'ƒê∆°n gi√° NT khai b√°o', 'ƒê∆°n gi√° ƒëi·ªÅu ch·ªânh(USD)', 'ƒê∆°n gi√° NT ƒëi·ªÅu ch·ªânh', 'PP khai b√°o', 'PP √°p gi√°', 'Nguy√™n t·ªá', 'T·ª∑ gi√° nguy√™n t·ªá', 'T·ª∑ gi√° USD', 'L∆∞·ª£ng', 'ƒê∆°n v·ªã t√≠nh', 'T√™n nu·ªõc xu·∫•t x·ª©', 'S·ªë h·ª£p ƒë·ªìng', 'Ng√†y h·ª£p ƒë·ªìng', 'M√£ ph∆∞∆°ng th·ª©c thanh to√°n', 'ƒêi·ªÅu ki·ªán giao h√†ng', 'M√£ ph∆∞∆°ng ti·ªán v·∫≠n chuy·ªÉn', 'Ph∆∞∆°ng ti·ªán v·∫≠n chuy·ªÉn', 'Ki·ªÉm tra gi√° c·∫•p T·ªïng c·ª•c', 'Ki·ªÉm tra gi√° c·∫•p C·ª•c', 'Ki·ªÉm tra gi√° c·∫•p Chi c·ª•c', 'Thu·∫ø su·∫•t XNK', 'Thu·∫ø su·∫•t TTƒêB', 'Thu·∫ø su·∫•t VAT', 'Thu·∫ø su·∫•t t·ª± v·ªá', 'Thu·∫ø XNK', 'Thu·∫ø TTƒêB', 'Thu·∫ø VAT', 'Thu·∫ø m√¥i tr∆∞·ªùng', 'Thu·∫ø t·ª± v·ªá', 'X·∫øp h·∫°ng DN', 'Tr·∫°ng th√°i', 'K·∫øt qu·∫£ ki·ªÉm tra STQ', 'N∆∞·ªõc nh·∫≠p kh·∫©u', 'Ng√†y KTG chi c·ª•c', 'K·∫øt qu·∫£ TVG chi c·ª•c', 'K·∫øt qu·∫£ TVG c·ª•c']\n",
      "   ‚û§ Sheet: 0101684081\n",
      "   ‚û§ Sheet: 0101688110\n",
      "   ‚û§ Sheet: 0102277917\n",
      "   ‚û§ Sheet: 0104731808\n",
      "   ‚û§ Sheet: 0107284474\n",
      "   ‚û§ Sheet: 0109610652\n",
      "   ‚û§ Sheet: 0200135942\n",
      "   ‚û§ Sheet: 0301491030\n",
      "   ‚û§ Sheet: 0302237742\n",
      "   ‚û§ Sheet: 0302606809\n",
      "   ‚û§ Sheet: 0302764114\n",
      "   ‚û§ Sheet: 0302887282\n",
      "   ‚û§ Sheet: 0303181323\n",
      "   ‚û§ Sheet: 0303241445\n",
      "   ‚û§ Sheet: 0303270679\n",
      "   ‚û§ Sheet: 0304253563\n",
      "   ‚û§ Sheet: 0304433710\n",
      "   ‚û§ Sheet: 0304548905\n",
      "   ‚û§ Sheet: 0304836639\n",
      "   ‚û§ Sheet: 0304918352\n",
      "   ‚û§ Sheet: 0304933135\n",
      "   ‚û§ Sheet: 0304973949\n",
      "   ‚û§ Sheet: 0305310694\n",
      "   ‚û§ Sheet: 0307925326\n",
      "   ‚û§ Sheet: 0308929098\n",
      "   ‚û§ Sheet: 0308995196\n",
      "   ‚û§ Sheet: 0309794559\n",
      "   ‚û§ Sheet: 0310474190\n",
      "   ‚û§ Sheet: 0310498843\n",
      "   ‚û§ Sheet: 0311860897\n",
      "   ‚û§ Sheet: 0312473298\n",
      "   ‚û§ Sheet: 0314143062\n",
      "   ‚û§ Sheet: 0317029689\n",
      "   ‚û§ Sheet: 04081100\n",
      "   ‚û§ Sheet: 04089100\n",
      "   ‚û§ Sheet: 130239\n",
      "   ‚û§ Sheet: 13023990\n",
      "   ‚û§ Sheet: 17029099\n",
      "   ‚û§ Sheet: 21021000\n",
      "   ‚û§ Sheet: 21069041\n",
      "   ‚û§ Sheet: 21069049\n",
      "   ‚û§ Sheet: 23099020\n",
      "   ‚û§ Sheet: 28273990\n",
      "   ‚û§ Sheet: 29071900\n",
      "   ‚û§ Sheet: 29161900\n",
      "   ‚û§ Sheet: 29181900\n",
      "   ‚û§ Sheet: 29211900\n",
      "   ‚û§ Sheet: 29242910\n",
      "   ‚û§ Sheet: 29321400\n",
      "   ‚û§ Sheet: 29322090\n",
      "   ‚û§ Sheet: 29349910\n",
      "   ‚û§ Sheet: 29349990\n",
      "   ‚û§ Sheet: 29400000\n",
      "   ‚û§ Sheet: 3204\n",
      "   ‚û§ Sheet: 32041710\n",
      "   ‚û§ Sheet: 32041900\n",
      "   ‚û§ Sheet: 330190\n",
      "   ‚û§ Sheet: 350300\n",
      "   ‚û§ Sheet: 35040000\n",
      "   ‚û§ Sheet: 3700720496\n",
      "   ‚û§ Sheet: 39123100\n",
      "\n",
      "üîπ Nh√≥m 3 - Header: ['T·ªù khai', 'Ng√†y ƒëƒÉng k√Ω', 'T√™n n∆°i m·ªü t·ªù khai', 'M√£ doanh nghi·ªáp XNK', 'T√™n doanh nghi·ªáp XNK', 'CTY Nh·∫≠p G·ªôp', 'ƒê∆°n v·ªã ƒë·ªëi t√°c', 'nccg', 'M√£ h√†ng khai b√°o', 'S·ªë th·ª© t·ª± h√†ng', 'T√™n h√†ng', 'THG', 'H·ª£p l·ªá', 'ƒê∆°n gi√° khai b√°o(USD)', 'ƒê∆°n gi√° NT khai b√°o', 'ƒê∆°n gi√° ƒëi·ªÅu ch·ªânh(USD)', 'ƒê∆°n gi√° NT ƒëi·ªÅu ch·ªânh', 'PP khai b√°o', 'PP √°p gi√°', 'Nguy√™n t·ªá', 'T·ª∑ gi√° nguy√™n t·ªá', 'T·ª∑ gi√° USD', 'Kh·ªëi l∆∞·ª£ng gen', 'L∆∞·ª£ng', 'ƒê∆°n v·ªã t√≠nh', 'T√™n nu·ªõc xu·∫•t x·ª©', 'S·ªë h·ª£p ƒë·ªìng', 'Ng√†y h·ª£p ƒë·ªìng', 'M√£ ph∆∞∆°ng th·ª©c thanh to√°n', 'ƒêi·ªÅu ki·ªán giao h√†ng', 'M√£ ph∆∞∆°ng ti·ªán v·∫≠n chuy·ªÉn', 'Ph∆∞∆°ng ti·ªán v·∫≠n chuy·ªÉn', 'Ki·ªÉm tra gi√° c·∫•p T·ªïng c·ª•c', 'Ki·ªÉm tra gi√° c·∫•p C·ª•c', 'Ki·ªÉm tra gi√° c·∫•p Chi c·ª•c', 'Thu·∫ø su·∫•t XNK', 'Thu·∫ø su·∫•t TTƒêB', 'Thu·∫ø su·∫•t VAT', 'Thu·∫ø su·∫•t t·ª± v·ªá', 'Thu·∫ø XNK', 'Thu·∫ø TTƒêB', 'Thu·∫ø VAT', 'Thu·∫ø m√¥i tr∆∞·ªùng', 'Thu·∫ø t·ª± v·ªá', 'X·∫øp h·∫°ng DN', 'Tr·∫°ng th√°i', 'K·∫øt qu·∫£ ki·ªÉm tra STQ', 'N∆∞·ªõc nh·∫≠p kh·∫©u', 'Ng√†y KTG chi c·ª•c', 'K·∫øt qu·∫£ TVG chi c·ª•c', 'K·∫øt qu·∫£ TVG c·ª•c', 'diplicate']\n",
      "   ‚û§ Sheet: 29321400.xls\n",
      "\n",
      "‚ö†Ô∏è C√≥ s·ª± kh√°c bi·ªát v·ªÅ ti√™u ƒë·ªÅ c·ªôt gi·ªØa c√°c sheet!\n"
     ]
    }
   ],
   "source": [
    "link = \"../../Data/Du lieu hai quan/Thang 5 lan 2/thang5lan2.xlsx\"\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def phan_loai_sheet_theo_header(file_excel):\n",
    "    header_map = defaultdict(list)\n",
    "\n",
    "    # ƒê·ªçc t·∫•t c·∫£ sheet v√† gom nh√≥m theo header\n",
    "    xls = pd.ExcelFile(file_excel)\n",
    "    for sheet in xls.sheet_names:\n",
    "        try:\n",
    "            df = pd.read_excel(file_excel, sheet_name=sheet, nrows=0)\n",
    "            header = tuple(df.columns)  # d√πng tuple ƒë·ªÉ so s√°nh d·ªÖ h∆°n\n",
    "            header_map[header].append(sheet)\n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói khi ƒë·ªçc sheet {sheet}: {e}\")\n",
    "\n",
    "    # In ra c√°c nh√≥m sheet theo header\n",
    "    print(\"üìä K·∫øt qu·∫£ ph√¢n lo·∫°i sheet theo ti√™u ƒë·ªÅ c·ªôt:\\n\")\n",
    "    for i, (header, sheets) in enumerate(header_map.items(), 1):\n",
    "        print(f\"üîπ Nh√≥m {i} - Header: {list(header)}\")\n",
    "        for s in sheets:\n",
    "            print(f\"   ‚û§ Sheet: {s}\")\n",
    "        print()\n",
    "\n",
    "    # N·∫øu c√≥ nhi·ªÅu nh√≥m ‚Üí c√≥ s·ª± kh√°c bi·ªát\n",
    "    if len(header_map) > 1:\n",
    "        print(\"‚ö†Ô∏è C√≥ s·ª± kh√°c bi·ªát v·ªÅ ti√™u ƒë·ªÅ c·ªôt gi·ªØa c√°c sheet!\")\n",
    "    else:\n",
    "        print(\"‚úÖ T·∫•t c·∫£ c√°c sheet ƒë·ªÅu c√≥ ti√™u ƒë·ªÅ c·ªôt gi·ªëng nhau.\")\n",
    "\n",
    "# G·ªçi h√†m\n",
    "phan_loai_sheet_theo_header(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3bcf770",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_out = \"out_data_last_052025.xlsx\"\n",
    "# df = pd.read_excel(\"../back-up-code-bfc/file_name_out\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c781ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def gop_cac_sheet_trong_1_file(file_excel, file_luu=file_name_out):\n",
    "    xls = pd.ExcelFile(file_excel)\n",
    "    ds_sheet = xls.sheet_names\n",
    "    tat_ca_du_lieu = []\n",
    "\n",
    "    for sheet in ds_sheet:\n",
    "        try:\n",
    "            df = pd.read_excel(file_excel, sheet_name=sheet)\n",
    "            df[\"T√™n Sheet\"] = sheet  # th√™m c·ªôt t√™n sheet\n",
    "            tat_ca_du_lieu.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"L·ªói ƒë·ªçc sheet {sheet}: {e}\")\n",
    "\n",
    "    if tat_ca_du_lieu:\n",
    "        df_gop = pd.concat(tat_ca_du_lieu, ignore_index=True)\n",
    "        df_gop.to_excel(file_luu, sheet_name=\"TongHop\", index=False)\n",
    "        print(f\"‚úÖ ƒê√£ g·ªôp t·∫•t c·∫£ sheet v√†o file '{file_luu}'\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o ƒë·ªÉ g·ªôp.\")\n",
    "        \n",
    "    print(df_gop[\"T√™n Sheet\"].nunique())\n",
    "\n",
    "    df_gop.to_excel(file_luu, sheet_name=\"TongHop\", index=False)\n",
    "    print(f\"‚úÖ ƒê√£ g·ªôp t·∫•t c·∫£ sheet v√†o file '{file_luu}'\")\n",
    "\n",
    "\n",
    "# G·ªçi h√†m\n",
    "gop_cac_sheet_trong_1_file(link)\n",
    "# print(f\"üìÇ File ƒë√£ l∆∞u t·∫°i: {link_df}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf93595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_clean = df.copy()\n",
    "# # Assuming 'T·ªù khai 11 s·ªë' is a string column\n",
    "# df_to_clean['T·ªù khai 11 s·ªë'] = df_to_clean['T·ªù khai'].astype(str).str[:11] + df_to_clean['L∆∞·ª£ng'].astype(str) + df_to_clean['ƒê∆°n gi√° NT khai b√°o'].astype(str)\n",
    "\n",
    "\n",
    "# # Create a boolean mask for duplicated rows\n",
    "# duplicated_rows = df_to_clean.duplicated(subset='T·ªù khai 11 s·ªë', keep='first')\n",
    "\n",
    "# # Update 'Duplicated_MTK' column for rows beyond the first occurrence\n",
    "# df_to_clean['is_duplicated']= duplicated_rows.astype(int)\n",
    "# df_to_clean = df_to_clean.drop(['T·ªù khai 11 s·ªë'], axis=1)\n",
    "\n",
    "# # Update the first occurrence as 0\n",
    "# df_to_clean.loc[duplicated_rows, 'is_duplicated'] = 1\n",
    "# # df.to_excel(\"{}/{} {} renamed.xlsx\".format(chat, chat, time), index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83f41d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['T√™n Sheet', 'T·ªù khai', 'Ng√†y ƒëƒÉng k√Ω', 'T√™n n∆°i m·ªü t·ªù khai',\n",
      "       'M√£ doanh nghi·ªáp XNK', 'T√™n doanh nghi·ªáp XNK', 'ƒê∆°n v·ªã ƒë·ªëi t√°c',\n",
      "       'M√£ h√†ng khai b√°o', 'S·ªë th·ª© t·ª± h√†ng', 'T√™n h√†ng',\n",
      "       'ƒê∆°n gi√° khai b√°o(USD)', 'ƒê∆°n gi√° NT khai b√°o',\n",
      "       'ƒê∆°n gi√° ƒëi·ªÅu ch·ªânh(USD)', 'ƒê∆°n gi√° NT ƒëi·ªÅu ch·ªânh', 'PP khai b√°o',\n",
      "       'PP √°p gi√°', 'Nguy√™n t·ªá', 'T·ª∑ gi√° nguy√™n t·ªá', 'T·ª∑ gi√° USD', 'L∆∞·ª£ng',\n",
      "       'ƒê∆°n v·ªã t√≠nh', 'T√™n nu·ªõc xu·∫•t x·ª©', 'S·ªë h·ª£p ƒë·ªìng', 'Ng√†y h·ª£p ƒë·ªìng',\n",
      "       'M√£ ph∆∞∆°ng th·ª©c thanh to√°n', 'ƒêi·ªÅu ki·ªán giao h√†ng',\n",
      "       'M√£ ph∆∞∆°ng ti·ªán v·∫≠n chuy·ªÉn', 'Ph∆∞∆°ng ti·ªán v·∫≠n chuy·ªÉn',\n",
      "       'Ki·ªÉm tra gi√° c·∫•p T·ªïng c·ª•c', 'Ki·ªÉm tra gi√° c·∫•p C·ª•c',\n",
      "       'Ki·ªÉm tra gi√° c·∫•p Chi c·ª•c', 'Thu·∫ø su·∫•t XNK', 'Thu·∫ø su·∫•t TTƒêB',\n",
      "       'Thu·∫ø su·∫•t VAT', 'Thu·∫ø su·∫•t t·ª± v·ªá', 'Thu·∫ø XNK', 'Thu·∫ø TTƒêB', 'Thu·∫ø VAT',\n",
      "       'Thu·∫ø m√¥i tr∆∞·ªùng', 'Thu·∫ø t·ª± v·ªá', 'X·∫øp h·∫°ng DN', 'Tr·∫°ng th√°i',\n",
      "       'K·∫øt qu·∫£ ki·ªÉm tra STQ', 'N∆∞·ªõc nh·∫≠p kh·∫©u', 'Ng√†y KTG chi c·ª•c',\n",
      "       'K·∫øt qu·∫£ TVG chi c·ª•c', 'K·∫øt qu·∫£ TVG c·ª•c'],\n",
      "      dtype='object')\n",
      "Index(['T√™n Sheet', 'T·ªù khai', 'Ng√†y ƒëƒÉng k√Ω', 'T√™n n∆°i m·ªü t·ªù khai',\n",
      "       'M√£ doanh nghi·ªáp XNK', 'T√™n doanh nghi·ªáp XNK', 'ƒê∆°n v·ªã ƒë·ªëi t√°c',\n",
      "       'M√£ h√†ng khai b√°o', 'S·ªë th·ª© t·ª± h√†ng', 'T√™n h√†ng',\n",
      "       'ƒê∆°n gi√° khai b√°o(USD)', 'ƒê∆°n gi√° NT khai b√°o',\n",
      "       'ƒê∆°n gi√° ƒëi·ªÅu ch·ªânh(USD)', 'ƒê∆°n gi√° NT ƒëi·ªÅu ch·ªânh', 'PP khai b√°o',\n",
      "       'PP √°p gi√°', 'Nguy√™n t·ªá', 'T·ª∑ gi√° nguy√™n t·ªá', 'T·ª∑ gi√° USD', 'L∆∞·ª£ng',\n",
      "       'ƒê∆°n v·ªã t√≠nh', 'T√™n nu·ªõc xu·∫•t x·ª©', 'S·ªë h·ª£p ƒë·ªìng', 'Ng√†y h·ª£p ƒë·ªìng',\n",
      "       'M√£ ph∆∞∆°ng th·ª©c thanh to√°n', 'ƒêi·ªÅu ki·ªán giao h√†ng',\n",
      "       'M√£ ph∆∞∆°ng ti·ªán v·∫≠n chuy·ªÉn', 'Ph∆∞∆°ng ti·ªán v·∫≠n chuy·ªÉn',\n",
      "       'Ki·ªÉm tra gi√° c·∫•p T·ªïng c·ª•c', 'Ki·ªÉm tra gi√° c·∫•p C·ª•c',\n",
      "       'Ki·ªÉm tra gi√° c·∫•p Chi c·ª•c', 'Thu·∫ø su·∫•t XNK', 'Thu·∫ø su·∫•t TTƒêB',\n",
      "       'Thu·∫ø su·∫•t VAT', 'Thu·∫ø su·∫•t t·ª± v·ªá', 'Thu·∫ø XNK', 'Thu·∫ø TTƒêB', 'Thu·∫ø VAT',\n",
      "       'Thu·∫ø m√¥i tr∆∞·ªùng', 'Thu·∫ø t·ª± v·ªá', 'X·∫øp h·∫°ng DN', 'Tr·∫°ng th√°i',\n",
      "       'K·∫øt qu·∫£ ki·ªÉm tra STQ', 'N∆∞·ªõc nh·∫≠p kh·∫©u', 'Ng√†y KTG chi c·ª•c',\n",
      "       'K·∫øt qu·∫£ TVG chi c·ª•c', 'K·∫øt qu·∫£ TVG c·ª•c', 'Kh·ªëi l∆∞·ª£ng c·∫≠p nh·∫≠t'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# C·∫≠p nh·∫≠t b·∫£ng kh·ªëi l∆∞·ª£ng s·∫£n ph·∫©m\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_conversion_rate(description):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t t·ª∑ l·ªá chuy·ªÉn ƒë·ªïi t·ª´ m√¥ t·∫£ s·∫£n ph·∫©m.\n",
    "    T√¨m ki·∫øm pattern: \"1 unit = 0.847 kgs\"\n",
    "    \"\"\"\n",
    "    match = re.search(r'1\\s*unit\\s*=\\s*(\\d+(?:\\.\\d+)?)\\s*(kg|kgs|kilogram|grams|g|ton|pound|lb|ounce|oz|UNIT)', description, re.IGNORECASE)\n",
    "    if match:\n",
    "        value = float(match.group(1))\n",
    "        unit = match.group(2).lower()\n",
    "        unit_mapping = {\n",
    "            'kilogram': 1, 'kilograms': 1, 'kgm': 1, 'kg': 1, 'kgs': 1,\n",
    "        'gram': 0.001, 'g': 0.001,'GRM': 0.001,\n",
    "        'ton': 1000, 'cubic meter': 1000, 'metric ton': 1000, 'tonne': 1000,\n",
    "        'pound': 0.453592, 'lb': 0.453592,\n",
    "        'ounce': 0.0283495, 'oz': 0.0283495,\n",
    "        'ml': 0.001, 'milliliter': 0.001, 'TNE': 1000\n",
    "        }\n",
    "        return value * unit_mapping.get(unit, 1)\n",
    "    return None\n",
    "\n",
    "def update_quantities(row):\n",
    "    unit_mapping = {\n",
    "        'kilogram': 1, 'kilograms': 1, 'kgm': 1, 'kg': 1, 'kgs': 1,\n",
    "        'gram': 0.001, 'g': 0.001,'GRM': 0.001,\n",
    "        'ton': 1000, 'cubic meter': 1000, 'metric ton': 1000, 'tonne': 1000,\n",
    "        'pound': 0.453592, 'lb': 0.453592,\n",
    "        'ounce': 0.0283495, 'oz': 0.0283495,\n",
    "        'ml': 0.001, 'milliliter': 0.001, 'TNE': 1000\n",
    "    }\n",
    "    \n",
    "    unit = row['ƒê∆°n v·ªã t√≠nh'].strip().lower()\n",
    "    quantity = row['L∆∞·ª£ng']\n",
    "    \n",
    "    if unit in unit_mapping:\n",
    "        return quantity * unit_mapping[unit]\n",
    "    else:\n",
    "        conversion_rate = extract_conversion_rate(row['T√™n h√†ng'])\n",
    "        if conversion_rate:\n",
    "            return quantity * conversion_rate\n",
    "    \n",
    "    return quantity  # Gi·ªØ nguy√™n n·∫øu kh√¥ng t√¨m th·∫•y c√°ch quy ƒë·ªïi\n",
    "\n",
    "# V√≠ d·ª• s·ª≠ d·ª•ng v·ªõi pandas\n",
    "# df = pd.read_excel(\"../back-up-code-bfc/out_data_last_052025.xlsx\")\n",
    "df = pd.read_excel(f\"../back-up-code-bfc/{file_name_out}\")\n",
    "# df = pd.read_excel(f\"{link_df}\") \n",
    "data_for_valid = df.copy()\n",
    "print(data_for_valid.columns)\n",
    "data_for_valid['Kh·ªëi l∆∞·ª£ng c·∫≠p nh·∫≠t'] = data_for_valid.apply(update_quantities, axis=1)\n",
    "print(data_for_valid.columns)\n",
    "data_for_valid.to_excel(f'../../Data/Du lieu hai quan/dulieuhaiquanout/{file_name_out}', index=False)\n",
    "# Save to a valid file path (ensure the folder exists and you have write permission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da94919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doan Duy Thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.0 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Doan Duy Thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.0 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    # Remove non-alphabetic characters, keeping both English and Vietnamese alphabets\n",
    "    text = re.sub(\"[^a-zA-Z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖƒë√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπ]\", \" \", text)\n",
    "    # Remove specified words\n",
    "    exclusion_list = []\n",
    "\n",
    "    # Remove weight units\n",
    "    weight_units = [\"kilogram\",\"kilograms\",\"kg\",\"kgs\",\"kgm\",\"kgms\",\"gm\",\"gms\",\"g\", \"gram\", \"grams\", \"ml\",\n",
    "                    \"lb\", \"pound\", \"pounds\", \"oz\", \"ounce\", \"ounces\", \"unit\", \"units\",\"pce\",\"ton\",\"cubic meter\",\n",
    "                    \"tne\",\"ton\",\"milligram\",\"mg\",\"microgram\",\"¬µg\",\"metric ton\",\"tonne\",\"stone\",\"st\",\"cara\",\"car\",\n",
    "                    \"ct\",\"grain\",\"gr\",\"pce\", 'lit']\n",
    "    exclusion_list += weight_units\n",
    "    \n",
    "    text = \" \".join(word for word in text.split() if word.lower() not in exclusion_list)\n",
    "    return text\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "classifier = CatBoostClassifier()\n",
    "classifier.load_model('./Model AI/ingre_w_all_CatBoosting_model')\n",
    "\n",
    "tfidf_vectorizer = joblib.load('./Model AI/ingre_w_all_fidf_vectorizer.joblib')\n",
    "tfidf_vectorizer.preprocessor = custom_preprocessor  # Set the custom preprocessor\n",
    "\n",
    "# create validity column\n",
    "data_for_valid['T√™n h√†ng'] = data_for_valid['T√™n h√†ng'].str.lower()\n",
    "product_des = tfidf_vectorizer.transform(data_for_valid[\"T√™n h√†ng\"]).toarray()\n",
    "data_for_valid['S·∫£n ph·∫©m'] = \"\"\n",
    "ingre = data_for_valid['S·∫£n ph·∫©m'].astype(str).fillna('NaN').values.reshape(-1,1)\n",
    "\n",
    "# price = data_for_valid['Th√†nh_ti·ªÅn'].values.reshape(-1,1)\n",
    "# X_new = np.concatenate((price, ingre, product_des), axis=1)\n",
    "X_new = np.concatenate((ingre, product_des), axis=1)\n",
    "\n",
    "\n",
    "data_for_valid['H·ª£p_l·ªá'] = classifier.predict(X_new)\n",
    "data_for_valid.insert(data_for_valid.columns.get_loc('T√™n h√†ng') + 1, 'H·ª£p_l·ªá', data_for_valid.pop('H·ª£p_l·ªá'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e859e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from brand_and_updated_quant import find_company_name\n",
    "data_for_valid['Brand'] = data_for_valid['T√™n h√†ng'].apply(find_company_name)\n",
    "data_for_valid.insert(data_for_valid.columns.get_loc('T√™n h√†ng') + 1, 'Brand', data_for_valid.pop('Brand'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d680690",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_valid.to_excel(f\"../../Data/Du lieu hai quan/dulieuhaiquanout/{file_name_out}\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
