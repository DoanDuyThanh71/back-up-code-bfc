# %load_ext autoreload
# %autoreload
import pandas as pd
import re
import numpy as np
from  brand_and_updated_quant import find_company_name, update_quantities
import joblib
from merge_files import merge_ingredients, convert_x2x, str_to_bool
import os

newFileName = "smock sauge gia v·ªã x√∫c x√≠ch" 

downloaded_folder = "./Downloads/"

# convert xls to xlsx files
download_directory = os.path.abspath(downloaded_folder)
convert_x2x(download_directory+'/')

# List all files in the download directory
files = [f for f in os.listdir(download_directory) if f.endswith('.xlsx')]

# Read each file and print the number of columns
for file in files:  
    file_path = os.path.join(download_directory, file)
    df = pd.read_excel(file_path)
    print(f"{file}: {df.shape[1]} columns")

# convert xls to xlsx files
download_directory = os.path.abspath(downloaded_folder)
convert_x2x(download_directory+'/')

# List all files in the download directory
files = [f for f in os.listdir(download_directory) if f.endswith('.xlsx')]

# Read each file and print the number of columns




# export_data = input("Enter True or False: ")
export_data = "False"
try:
    export_data = str_to_bool(export_data)
    print(f"The boolean value is: {export_data}")
except ValueError as e:
    print(e)
    



df = merge_ingredients(downloaded_folder+"/", export_data=export_data) 
df.head(5)

df.shape

df_to_clean =df.copy()
df_to_clean.head(5)

# Convert the 'Date' column to datetime if it's not already
df_to_clean['Date'] = pd.to_datetime(df_to_clean['Date'], format='%Y-%m-%d')

# Extract day, month, and year into separate columns
df_to_clean['Day'] = df_to_clean['Date'].dt.day
df_to_clean['Month'] = df_to_clean['Date'].dt.month
df_to_clean['Year'] = df_to_clean['Date'].dt.year

df_to_clean.insert(df_to_clean.columns.get_loc('Date')+1, 'Day', df_to_clean.pop('Day'))
df_to_clean.insert(df_to_clean.columns.get_loc('Date')+2, 'Month', df_to_clean.pop('Month'))
df_to_clean.insert(df_to_clean.columns.get_loc('Date')+3, 'Year', df_to_clean.pop('Year'))

# Assuming 'M√£_t·ªù_khai 11 s·ªë' is a string column
df_to_clean['M√£_t·ªù_khai 11 s·ªë'] = df_to_clean['M√£_t·ªù_khai'].astype(str).str[:11] + df_to_clean['S·ªë_l∆∞·ª£ng'].astype(str) + df_to_clean['Th√†nh_ti·ªÅn'].astype(str)


# Create a boolean mask for duplicated rows
duplicated_rows = df_to_clean.duplicated(subset='M√£_t·ªù_khai 11 s·ªë', keep='first')

# Update 'Duplicated_MTK' column for rows beyond the first occurrence
df_to_clean['is_duplicated']= duplicated_rows.astype(int)
df_to_clean = df_to_clean.drop(['M√£_t·ªù_khai 11 s·ªë'], axis=1)

# Update the first occurrence as 0
df_to_clean.loc[duplicated_rows, 'is_duplicated'] = 1
# df.to_excel("{}/{} {} renamed.xlsx".format(chat, chat, time), index=False, header=True)

# Calculate updated price based on Th√†nh_ti·ªÅn / Updated_S·ªë_l∆∞·ª£ng

# Define a function to apply
def calculate_updated_ƒë∆°n_gi√°(row):
    if row['Updated_S·ªë_l∆∞·ª£ng'] != 0:
        return row['Th√†nh_ti·ªÅn'] / row['Updated_S·ªë_l∆∞·ª£ng']
    else:
        return 0  # Return 0 if Updated_S·ªë_l∆∞·ª£ng is 0 to avoid division by zero
    
def extract_after_keywords(text):
    keywords = ["t·ªïng h·ª£p","qu·ªëc t·∫ø","d·ªãch v·ª•","th∆∞∆°ng m·∫°i", "TM", "s·∫£n xu·∫•t", "c·ªï ph·∫ßn", "MTV", "m·ªôt th√†nh vi√™n", "TNHH", "tr√°ch nhi·ªám h·ªØu h·∫°n"]
    
    # Convert text to lowercase for case-insensitive search
    text_lower = text.lower()
    if text_lower.find("nh√† ga"):
        keywords.pop(0)
    for keyword in keywords:
        keyword_lower = keyword.lower()
        if keyword_lower in text_lower:
            start_index = text_lower.find(keyword_lower) + len(keyword_lower)
            result = text[start_index:].strip().replace("XU·∫§T NH·∫¨P KH·∫®U", "XNK")
            return result
    
    return text # Return None if none of the keywords are found

# Example usage
text = "C√¥ng ty TNHH nh√† ga qu·ªëc t·∫ø ABC"
result = extract_after_keywords(text)
print(result)  # Output: "ABC"


from brand_and_updated_quant import find_company_name, update_quantities
# Make a safepoint
data_for_valid = df_to_clean.copy()

# data_for_valid['H·ª£p_l·ªá'] = data_for_valid['H·ª£p_l·ªá'].map({1: "c√≥", 0: "kh√¥ng"})

# data_for_valid = valid_df.copy()
data_for_valid['Brand'] = data_for_valid['M√¥_t·∫£_s·∫£n_ph·∫©m'].apply(find_company_name)
data_for_valid.insert(data_for_valid.columns.get_loc('M√¥_t·∫£_s·∫£n_ph·∫©m') + 1, 'Brand', data_for_valid.pop('Brand'))

# Add updated quantity and unit columns
data_for_valid['Updated_S·ªë_l∆∞·ª£ng'] = data_for_valid.apply(update_quantities, axis=1)
data_for_valid['Updated_ƒê∆°n_v·ªã'] = ["kilogram"] * len(data_for_valid)
data_for_valid.insert(data_for_valid.columns.get_loc('Brand') + 1, 'Updated_S·ªë_l∆∞·ª£ng', data_for_valid.pop('Updated_S·ªë_l∆∞·ª£ng'))
data_for_valid.insert(data_for_valid.columns.get_loc('Brand') + 2, 'Updated_ƒê∆°n_v·ªã', data_for_valid.pop('Updated_ƒê∆°n_v·ªã'))

# Calculate updated price
data_for_valid['Updated_ƒê∆°n_gi√°'] = data_for_valid.apply(lambda row: calculate_updated_ƒë∆°n_gi√°(row), axis=1)
data_for_valid.insert(data_for_valid.columns.get_loc('ƒê∆°n_gi√°'), 'Updated_ƒê∆°n_gi√°', data_for_valid.pop('Updated_ƒê∆°n_gi√°'))

# Vlookup for market classification
df_marketclass = pd.read_excel('../../Data/Data for fill data/Updated dim_company_marketType.xlsx', sheet_name='ph√¢n lo·∫°i th·ªã tr∆∞·ªùng')
df_marketclass.columns = df_marketclass.columns.str.strip()  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi n·∫øu c√≥
# Ensure that the column 'C√¥ng_ty_nh·∫≠p' exists in df_marketclass
print(df_marketclass.columns)
df_marketclass['C√¥ng_ty_nh·∫≠p'] = df_marketclass['C√¥ng_ty_nh·∫≠p'].fillna('Unknown')  # Thay NaN b·∫±ng m·ªôt gi√° tr·ªã m·∫∑c ƒë·ªãnh


# Ensure 'export_data' is defined and has the correct value
if export_data == False:
    company = 'C√¥ng_ty_nh·∫≠p'
    parent_company = 'C√¥ng ty nh·∫≠p g·ªôp'
else:
    company = 'Nh√†_cung_c·∫•p'
    parent_company = 'Nh√† cung c·∫•p g·ªôp'

# Ensure that the 'company' column exists in 'data_for_valid'
if company not in data_for_valid.columns:
    raise KeyError(f"Column '{company}' does not exist in data_for_valid.")

# Perform the merge
lookup_data = data_for_valid[company]
lookup_data.name = company

# Ensure the column 'C√¥ng_ty_nh·∫≠p' exists in df_marketclass
if 'C√¥ng_ty_nh·∫≠p' not in df_marketclass.columns:
    raise KeyError("'C√¥ng_ty_nh·∫≠p' column is missing in df_marketclass.")

result_df = pd.merge(lookup_data, df_marketclass, on='C√¥ng_ty_nh·∫≠p', how='left')

# Ensure the length of result_df matches data_for_valid
result_df = result_df.iloc[:len(data_for_valid)]
result_df = result_df.set_index(data_for_valid.index)

# Set the index to match data_for_valid
result_df = result_df.set_index(data_for_valid.index)

# Add market classification to data_for_valid
comp_nice_name = company.lower().replace('_', " ")
data_for_valid['MarketClassification'] = result_df['MarketClassification']
data_for_valid[f'Ph√¢n lo·∫°i {comp_nice_name}'] = result_df['Ph√¢n lo·∫°i c√¥ng ty nh·∫≠p']
data_for_valid[parent_company] = result_df['C√¥ng ty nh·∫≠p g·ªôp']
data_for_valid.insert(data_for_valid.columns.get_loc(company) + 1, parent_company, data_for_valid.pop(parent_company))


# Apply the function to fill in the 'C√¥ng ty nh·∫≠p g·ªôp' column
data_for_valid[parent_company] = data_for_valid[parent_company].fillna(
    data_for_valid[company].apply(extract_after_keywords)
)





import matplotlib.pyplot as plt
test =data_for_valid[data_for_valid['Updated_ƒê∆°n_gi√°']<100]

# Plotting the distribution using Seaborn
plt.figure(figsize=(10, 6))
plt.boxplot(test['Updated_ƒê∆°n_gi√°'])
plt.title('Distribution of ƒê∆°n_gi√°')
plt.xlabel('ƒê∆°n_gi√°')
plt.ylabel('Frequency')
plt.grid(True)
# plt.show()


print(data_for_valid['Updated_ƒê∆°n_gi√°'].mode().iloc[0])
print(data_for_valid['Updated_ƒê∆°n_gi√°']) 

# Price tuning removed - keeping simple calculation: Th√†nh_ti·ªÅn / Updated_S·ªë_l∆∞·ª£ng
print("Price tuning feature has been removed")
print("Updated_ƒê∆°n_gi√° is now calculated as: Th√†nh_ti·ªÅn / Updated_S·ªë_l∆∞·ª£ng")

# Price tuning logic removed
# Updated_ƒê∆°n_gi√° is now calculated directly as Th√†nh_ti·ªÅn / Updated_S·ªë_l∆∞·ª£ng
# No additional price adjustments or bounds checking
print("‚úÖ Price tuning logic has been completely removed")
print("‚úÖ Updated_ƒê∆°n_gi√° calculation is now simplified to: Th√†nh_ti·ªÅn / Updated_S·ªë_l∆∞·ª£ng")



data_for_valid['Updated_ƒê∆°n_gi√°']



def custom_preprocessor(text):
    # Remove non-alphabetic characters, keeping both English and Vietnamese alphabets
    text = re.sub("[^a-zA-Z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖƒë√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπ]", " ", text)
    # Remove specified words
    exclusion_list = []

    # Remove weight units
    weight_units = ["kilogram","kilograms","kg","kgs","kgm","kgms","gm","gms","g", "gram", "grams", "ml",
                    "lb", "pound", "pounds", "oz", "ounce", "ounces", "unit", "units","pce","ton","cubic meter",
                    "tne","ton","milligram","mg","microgram","¬µg","metric ton","tonne","stone","st","cara","car",
                    "ct","grain","gr","pce", 'lit']
    exclusion_list += weight_units
    
    text = " ".join(word for word in text.split() if word.lower() not in exclusion_list)
    return text

from sklearn.feature_extraction.text import TfidfVectorizer
from catboost import CatBoostClassifier
classifier = CatBoostClassifier()
classifier.load_model('./models/Model AI/ingre_w_all_CatBoosting_model')

tfidf_vectorizer = joblib.load('./models/Model AI/ingre_w_all_fidf_vectorizer.joblib')
tfidf_vectorizer.preprocessor = custom_preprocessor  # Set the custom preprocessor

# create validity column
data_for_valid['M√¥_t·∫£_s·∫£n_ph·∫©m'] = data_for_valid['M√¥_t·∫£_s·∫£n_ph·∫©m'].str.lower()
product_des = tfidf_vectorizer.transform(data_for_valid["M√¥_t·∫£_s·∫£n_ph·∫©m"]).toarray()

# Check if 'S·∫£n ph·∫©m' column exists, if not create it with empty string
if 'S·∫£n ph·∫©m' not in data_for_valid.columns:
    data_for_valid['S·∫£n ph·∫©m'] = ''
    print("‚ö†Ô∏è C·ªôt 'S·∫£n ph·∫©m' kh√¥ng t·ªìn t·∫°i, ƒë√£ t·∫°o c·ªôt tr·ªëng")

ingre = data_for_valid['S·∫£n ph·∫©m'].astype(str).fillna('NaN').values.reshape(-1,1)

# price = data_for_valid['Th√†nh_ti·ªÅn'].values.reshape(-1,1)
# X_new = np.concatenate((price, ingre, product_des), axis=1)
X_new = np.concatenate((ingre, product_des), axis=1)


data_for_valid['H·ª£p_l·ªá'] = classifier.predict(X_new)
data_for_valid.insert(data_for_valid.columns.get_loc('Brand') + 1, 'H·ª£p_l·ªá', data_for_valid.pop('H·ª£p_l·ªá'))

from sklearn.feature_extraction.text import TfidfVectorizer
from catboost import CatBoostClassifier
classifier = CatBoostClassifier()
# classifier.load_model('quantity_w_huong_CatBoosting_model')
classifier.load_model('./models/Model AI/price_w_all_ingredients_CatBoosting_model')

# tfidf_vectorizer = joblib.load('quantity_w_huong_tfidf_vectorizer.joblib')
tfidf_vectorizer = joblib.load('./models/Model AI/price_w_all_ingredients_tfidf_vectorizer.joblib')

tfidf_vectorizer.preprocessor = custom_preprocessor  # Set the custom preprocessor

# create validity column
data_for_valid['M√¥_t·∫£_s·∫£n_ph·∫©m'] = data_for_valid['M√¥_t·∫£_s·∫£n_ph·∫©m'].str.lower()
product_des = tfidf_vectorizer.transform(data_for_valid["M√¥_t·∫£_s·∫£n_ph·∫©m"]).toarray()

quantity = data_for_valid['S·ªë_l∆∞·ª£ng'].values.reshape(-1,1)
X_new = np.concatenate((quantity, product_des), axis=1)
# price = data_for_valid['Th√†nh_ti·ªÅn'].values.reshape(-1,1)
# X_new = np.concatenate((price, product_des), axis=1)


data_for_valid['H·ª£p_l·ªá'] = classifier.predict(X_new)
data_for_valid.insert(data_for_valid.columns.get_loc('Brand') + 1, 'H·ª£p_l·ªá', data_for_valid.pop('H·ª£p_l·ªá'))

# Mapping m√£ c·∫£ng sang t√™n c·∫£ng ƒë·∫ßy ƒë·ªß
def map_port_codes(port_code):
    """
    Mapping m√£ c·∫£ng sang t√™n c·∫£ng ƒë·∫ßy ƒë·ªß
    """
    port_mapping = {
        # C·∫£ng S√†i G√≤n
        'HQSGKV1': 'C·∫£ng S√†i G√≤n KV1',
        'HQSGKV2': 'C·∫£ng S√†i G√≤n KV2', 
        'HQSGKV3': 'C·∫£ng S√†i G√≤n KV3',
        'HQSGKV4': 'C·∫£ng S√†i G√≤n KV4',
        
        # C·∫£ng H·∫£i Ph√≤ng
        'HQHPKV1': 'C·∫£ng H·∫£i Ph√≤ng KV1',
        'HQHPKV2': 'C·∫£ng H·∫£i Ph√≤ng KV2',
        'HQHPKV3': 'C·∫£ng H·∫£i Ph√≤ng KV3',
        
        # C·∫£ng N·ªôi B√†i
        'HQCNC': 'C·∫£ng N·ªôi B√†i',
        
        # C·∫£ng Ph√∫ M·ªπ
        'HQCPNHCM': 'C·∫£ng Ph√∫ M·ªπ HCM',
        'HQCPNHN': 'C·∫£ng Ph√∫ M·ªπ H√† N·ªôi',
        
        # C·∫£ng ƒê√¨nh V≈©
        'HQDINHVU': 'C·∫£ng ƒê√¨nh V≈©',
        
        # C·∫£ng T√¢n S∆°n Nh·∫•t
        'HQTSNHAT': 'C·∫£ng T√¢n S∆°n Nh·∫•t',
        
        # C·∫£ng Ti√™n S∆°n
        'HQTIENSON': 'C·∫£ng Ti√™n S∆°n',
        
        # C·∫£ng ƒê√† N·∫µng
        'CKCDANANG': 'C·∫£ng ƒê√† N·∫µng',
        'HQCKCDNA': 'C·∫£ng ƒê√† N·∫µng',
        
        # C·∫£ng Quy Nhon
        'CKQUYNHON': 'C·∫£ng Quy Nhon',
        
        # C·∫£ng Lao B·∫£o
        'CKLAOBAO': 'C·∫£ng Lao B·∫£o',
        
        # C·∫£ng C√† Mau
        'HQCAMAU': 'C·∫£ng C√† Mau',
        
        # C·∫£ng Vinh
        'HQVINH': 'C·∫£ng Vinh',
        
        # C√°c c·∫£ng ƒë·ªãa ph∆∞∆°ng kh√°c
        'HQBACGIANG': 'C·∫£ng B·∫Øc Giang',
        'HQBACHN': 'C·∫£ng B·∫Øc Ninh',
        'HQBACNINH': 'C·∫£ng B·∫Øc Ninh',
        'HQBACTL': 'C·∫£ng B·∫Øc Th√°i Lan',
        'HQBTHUAN': 'C·∫£ng B√¨nh Thu·∫≠n',
        'HQCKCKYHA': 'C·∫£ng K·ª≥ H√†',
        'HQCOCNAM': 'C·∫£ng √î C·∫ßu Nam',
        'HQDALAT': 'C·∫£ng ƒê√† L·∫°t',
        'HQDONGDANG': 'C·∫£ng ƒê·ªìng ƒêƒÉng',
        'HQHAIDUONG': 'C·∫£ng H·∫£i D∆∞∆°ng',
        'HQHANAM': 'C·∫£ng H√† Nam',
        'HQHGCT': 'C·∫£ng H·ªìng Gai',
        'HQHOABINH': 'C·∫£ng H√≤a B√¨nh',
        'HQHOALAC': 'C·∫£ng H√≤a L·∫°c',
        'HQHUNGYEN': 'C·∫£ng H∆∞ng Y√™n',
        'HQHUUNGHI': 'C·∫£ng H·ªØu Ngh·ªã',
        'HQKCNQNGAI': 'C·∫£ng KCN Qu·∫£ng Ng√£i',
        'HQKCXKCNHP': 'C·∫£ng KCX KCN H·∫£i Ph√≤ng',
        'HQLTRUNG1': 'C·∫£ng L·∫°ng Trung 1',
        'HQLTRUNG2': 'C·∫£ng L·∫°ng Trung 2',
        'HQNAMDINH': 'C·∫£ng Nam ƒê·ªãnh',
        'HQNINHBINH': 'C·∫£ng Ninh B√¨nh',
        'HQNOIBAI': 'C·∫£ng N·ªôi B√†i',
        'HQQTLAOCAI': 'C·∫£ng Qu·ªëc t·∫ø L√†o Cai',
        'HQTANTHANH': 'C·∫£ng T√¢n Th√†nh',
        'HQTDMOT': 'C·∫£ng T√¢n ƒê·ªãnh M·ªôt',
        'HQTHAIBINH': 'C·∫£ng Th√°i B√¨nh',
        'HQTNGUYEN': 'C·∫£ng T√¢n Nguy√™n',
        'HQTTHUAN': 'C·∫£ng T√¢n Thu·∫≠n',
        'HQVIETTRI': 'C·∫£ng Vi·ªát Tr√¨',
        'HQVINHPHUC': 'C·∫£ng Vƒ©nh Ph√∫c',
        'HQYENPHONG': 'C·∫£ng Y√™n Phong',
        'HQYENVIEN': 'C·∫£ng Y√™n Vi√™n',
        
        # C√°c khu c√¥ng nghi·ªáp
        'KCNVNSGBD': 'KCN Vi·ªát Nam-Singapore',
        'KCNSTHANBD': 'KCN S√¥ng Than',
        'KCXLBINHDN': 'KCX L√™ B√¨nh ƒê√† N·∫µng',
        'KTMCNMBTN': 'KTM C√¥ng nghi·ªáp Mi·ªÅn B·∫Øc',
        
        # C√°c c·∫£ng kh√°c
        'BENTRELA': 'C·∫£ng B·∫øn Tre',
        'BIENHOADN': 'C·∫£ng Bi√™n H√≤a',
        'CAIMEPVT': 'C·∫£ng C√°i M√©p',
        'CAOLANHCDT': 'C·∫£ng Cao L√£nh',
        'CATLOVT': 'C·∫£ng C√°t L·ªü',
        'CKCAUTREO': 'C·∫£ng C·∫ßu Treo',
        'CKCCAMRANH': 'C·∫£ng Cam Ranh',
        'CKCLONGAN': 'C·∫£ng Long An',
        'CKHONGAI': 'C·∫£ng H·ªìng Ng√†i',
        'CKQTCHALO': 'C·∫£ng Qu·ªëc t·∫ø Cha Lo',
        'CNQUANGNAM': 'C·∫£ng Qu·∫£ng Nam',
        'CSANBAYVT': 'C·∫£ng S√¢n bay V≈©ng T√†u',
        'CTHANHBP': 'C·∫£ng Th√†nh B√¨nh Ph∆∞·ªõc',
        'CTHOPBD': 'C·∫£ng Th·ªç B√¨nh ƒê·ªãnh',
        'DNVMPBD': 'C·∫£ng ƒê√† N·∫µng VMP',
        'DNVTBANGTN': 'C·∫£ng ƒê√† N·∫µng VT B·∫±ng T√¢n',
        'DUCHOALA': 'C·∫£ng D·ª•c H√≤a',
        'KCNDANANG': 'KCN ƒê√† N·∫µng',
        'MYTHOLA': 'C·∫£ng M·ªπ Tho',
        'NTRACHDN': 'C·∫£ng N√∫i Th√†nh ƒê√† N·∫µng',
        'NVPMYBRVT': 'C·∫£ng NVP M·ªπ B√† R·ªãa V≈©ng T√†u',
        'SONGTHANBD': 'C·∫£ng S√¥ng Than B√¨nh D∆∞∆°ng',
        'TAYDOCT': 'C·∫£ng T√¢y ƒê√¥',
        'TNHATDN': 'C·∫£ng T√¢n Nh·∫≠t ƒê√† N·∫µng',
        'VHUONGBD': 'C·∫£ng V∆∞·ªùn H∆∞∆°ng B√¨nh D∆∞∆°ng',
        'VINHLONGCT': 'C·∫£ng Vƒ©nh Long',
    }
    
    return port_mapping.get(port_code, port_code)  # Tr·∫£ v·ªÅ m√£ g·ªëc n·∫øu kh√¥ng t√¨m th·∫•y

# C·∫≠p nh·∫≠t c·ªôt C·∫£ng nh·∫≠p v·ªõi t√™n c·∫£ng ƒë·∫ßy ƒë·ªß
print("üîÑ ƒêang c·∫≠p nh·∫≠t t√™n c·∫£ng nh·∫≠p...")
# Check if 'Êµ∑ÂÖ≥‰ª£ÁêÜ‰ª£Á†Å' column exists
if 'Êµ∑ÂÖ≥‰ª£ÁêÜ‰ª£Á†Å' in data_for_valid.columns:
    data_for_valid['C·∫£ng nh·∫≠p'] = data_for_valid['Êµ∑ÂÖ≥‰ª£ÁêÜ‰ª£Á†Å'].apply(map_port_codes)
else:
    print("‚ö†Ô∏è C·ªôt 'Êµ∑ÂÖ≥‰ª£ÁêÜ‰ª£Á†Å' kh√¥ng t·ªìn t·∫°i, s·ª≠ d·ª•ng c·ªôt 'C·∫£ng nh·∫≠p' hi·ªán c√≥")
    if 'C·∫£ng nh·∫≠p' not in data_for_valid.columns:
        data_for_valid['C·∫£ng nh·∫≠p'] = ''

# Hi·ªÉn th·ªã th·ªëng k√™
port_counts = data_for_valid['C·∫£ng nh·∫≠p'].value_counts()
print(f"üìä T·ªïng s·ªë c·∫£ng kh√°c nhau: {len(port_counts)}")
print(f"üìà Top 10 c·∫£ng ph·ªï bi·∫øn nh·∫•t:")
print(port_counts.head(10))

print("‚úÖ Ho√†n th√†nh c·∫≠p nh·∫≠t t√™n c·∫£ng nh·∫≠p!")

# =============================================================================
# C·∫¨P NH·∫¨T C√îNG TY NH·∫¨P G·ªòP V√Ä TH√äM C√ÅC C·ªòT M·ªöI
# =============================================================================

print("\nüîÑ ƒêang c·∫≠p nh·∫≠t c√¥ng ty nh·∫≠p g·ªôp v√† th√™m c√°c c·ªôt m·ªõi...")

# 1. ƒê·ªçc file dim cty nh·∫≠p ƒë·ªÉ merge
try:
    # ƒê∆∞·ªùng d·∫´n file dim cty nh·∫≠p
    dim_file_path = "dim_cty_nh·∫≠p.xlsx"
    
    if os.path.exists(dim_file_path):
        print(f"üìñ ƒêang ƒë·ªçc file dim cty nh·∫≠p: {dim_file_path}")
        df_dim = pd.read_excel(dim_file_path)
        df_dim.columns = df_dim.columns.str.strip()
        
        print(f"üìä File dim c√≥ {len(df_dim)} d√≤ng d·ªØ li·ªáu")
        print(f"üìã C√°c c·ªôt trong file dim: {list(df_dim.columns)}")
        
        # 2. Merge tr·ª±c ti·∫øp t·ª´ file dim sang c·ªôt "C√¥ng ty nh·∫≠p g·ªôp"
        print("üîÑ ƒêang merge c√¥ng ty nh·∫≠p g·ªôp...")
        
        # S·ª≠a encoding v√† chu·∫©n h√≥a t√™n c√¥ng ty trong file dim
        def fix_encoding(text):
            if pd.isna(text):
                return text
            
            # S·ª≠a c√°c k√Ω t·ª± b·ªã l·ªói encoding ph·ªï bi·∫øn
            replacements = {
                'C√£¬¥Ng Ty': 'C√îNG TY',
                'Tnhh': 'TNHH',
                'Vi√°¬ª‚Ä°T Nam': 'VI·ªÜT NAM',
                'C√°¬ª Ph√°¬∫¬ßn': 'C·ªî PH·∫¶N',
                'Th√¶¬∞√Ü¬°Ng': 'TH∆Ø∆†NG',
                'M√°¬∫¬°I': 'M·∫†I',
                'D√°¬ª‚ÄπCh': 'D·ªäCH',
                'V√°¬ª': 'V·ª§',
                'N√Ü¬∞√Å¬ª‚Ä∫C': 'N∆Ø·ªöC',
                'Gi√°¬∫¬£i': 'GI·∫¢I',
                'Kh√É¬°T': 'KH√ÅT',
                'COLA': 'COLA',
                'T√°¬∫¬°I': 'T·∫†I',
                '√Ñ\x90√É': 'ƒê√Ä',
                'N√Å¬∫¬µNG': 'N·∫¥NG',
                'CHI NH√É¬°NH': 'CHI NH√ÅNH',
                'C√É¬¥NG': 'C√îNG',
                'N√Ü¬∞√Å¬ª‚Ä∫C': 'N∆Ø·ªöC',
                'GI√Å¬∫¬£I': 'GI·∫¢I',
                'KH√É¬°T': 'KH√ÅT',
                'VI√Å¬ª‚Ä°T': 'VI·ªÜT',
                'T√Å¬∫¬°I': 'T·∫†I',
                '√Ñ\x90√É': 'ƒê√Ä',
                'N√Å¬∫¬µNG': 'N·∫¥NG'
            }
            
            text_str = str(text)
            for wrong, correct in replacements.items():
                text_str = text_str.replace(wrong, correct)
            
            # Chu·∫©n h√≥a: lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a v√† chuy·ªÉn th√†nh uppercase
            text_str = ' '.join(text_str.split()).upper()
            return text_str
        
        # Chu·∫©n h√≥a t√™n c√¥ng ty trong data_for_valid ƒë·ªÉ so s√°nh
        def normalize_company_name(text):
            if pd.isna(text):
                return text
            return ' '.join(str(text).split()).upper()
        
        # √Åp d·ª•ng s·ª≠a encoding cho c·ªôt CTY nh·∫≠p
        df_dim['CTY nh·∫≠p_fixed'] = df_dim['CTY nh·∫≠p'].apply(fix_encoding)
        
        # Chu·∫©n h√≥a t√™n c√¥ng ty trong data_for_valid
        data_for_valid['C√¥ng_ty_nh·∫≠p_normalized'] = data_for_valid['C√¥ng_ty_nh·∫≠p'].apply(normalize_company_name)
        
        # Merge d·ª±a v√†o t√™n c√¥ng ty ƒë√£ chu·∫©n h√≥a
        data_for_valid = data_for_valid.merge(
            df_dim[['CTY nh·∫≠p_fixed', 'ctynhapgop']], 
            left_on='C√¥ng_ty_nh·∫≠p_normalized', 
            right_on='CTY nh·∫≠p_fixed',
            how='left'
        )
        
        # ƒê·ªïi t√™n c·ªôt ctynhapgop th√†nh "C√¥ng ty nh·∫≠p g·ªôp" ƒë·ªÉ d·ªÖ s·ª≠ d·ª•ng
        data_for_valid['C√¥ng ty nh·∫≠p g·ªôp'] = data_for_valid['ctynhapgop']
        
        # X√≥a c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt
        data_for_valid = data_for_valid.drop(['CTY nh·∫≠p_fixed', 'ctynhapgop', 'C√¥ng_ty_nh·∫≠p_normalized'], axis=1, errors='ignore')
        
        # ƒê·∫øm s·ªë b·∫£n ghi ƒë∆∞·ª£c merge
        mapped_count = data_for_valid['C√¥ng ty nh·∫≠p g·ªôp'].notna().sum()
        total_count = len(data_for_valid)
        print(f"‚úÖ ƒê√£ merge {mapped_count}/{total_count} b·∫£n ghi ({mapped_count/total_count*100:.1f}%)")
        
        # 3. Th√™m c·ªôt "Nh√† Cung C·∫•p G·ªôp" sau c·ªôt "Nh√†_cung_c·∫•p"
        print("üîÑ ƒêang th√™m c·ªôt Nh√† Cung C·∫•p G·ªôp...")
        
        # Merge nh√† cung c·∫•p g·ªôp n·∫øu c√≥ trong file dim
        if 'Nh√† cung c·∫•p g·ªôp' in df_dim.columns:
            data_for_valid = data_for_valid.merge(
                df_dim[['Nh√†_cung_c·∫•p', 'Nh√† cung c·∫•p g·ªôp']], 
                on='Nh√†_cung_c·∫•p', 
                how='left'
            )
            print("‚úÖ ƒê√£ th√™m c·ªôt Nh√† Cung C·∫•p G·ªôp")
        else:
            # N·∫øu kh√¥ng c√≥ c·ªôt nh√† cung c·∫•p g·ªôp, t·∫°o c·ªôt tr·ªëng
            data_for_valid['Nh√† Cung C·∫•p G·ªôp'] = None
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'Nh√† cung c·∫•p g·ªôp' trong file dim, t·∫°o c·ªôt tr·ªëng")
        
        # 4. Th√™m c·ªôt "Th∆∞∆°ng hi·ªáu g·ªôp" sau c·ªôt "Brand"
        print("üîÑ ƒêang th√™m c·ªôt Th∆∞∆°ng hi·ªáu g·ªôp...")
        
        # Merge th∆∞∆°ng hi·ªáu g·ªôp n·∫øu c√≥ trong file dim
        if 'Th∆∞∆°ng hi·ªáu g·ªôp' in df_dim.columns:
            data_for_valid = data_for_valid.merge(
                df_dim[['Brand', 'Th∆∞∆°ng hi·ªáu g·ªôp']], 
                on='Brand', 
                how='left'
            )
            print("‚úÖ ƒê√£ th√™m c·ªôt Th∆∞∆°ng hi·ªáu g·ªôp")
        else:
            # N·∫øu kh√¥ng c√≥ c·ªôt th∆∞∆°ng hi·ªáu g·ªôp, t·∫°o c·ªôt tr·ªëng
            data_for_valid['Th∆∞∆°ng hi·ªáu g·ªôp'] = None
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'Th∆∞∆°ng hi·ªáu g·ªôp' trong file dim, t·∫°o c·ªôt tr·ªëng")
        
        # 5. S·∫Øp x·∫øp l·∫°i v·ªã tr√≠ c√°c c·ªôt
        print("üîÑ ƒêang s·∫Øp x·∫øp l·∫°i v·ªã tr√≠ c√°c c·ªôt...")
        
        # T√¨m v·ªã tr√≠ c·ªôt "Nh√†_cung_c·∫•p" v√† ch√®n "Nh√† Cung C·∫•p G·ªôp" sau n√≥
        ncc_position = data_for_valid.columns.get_loc('Nh√†_cung_c·∫•p')
        data_for_valid.insert(ncc_position + 1, 'Nh√† Cung C·∫•p G·ªôp', data_for_valid.pop('Nh√† Cung C·∫•p G·ªôp'))
        
        # T√¨m v·ªã tr√≠ c·ªôt "Brand" v√† ch√®n "Th∆∞∆°ng hi·ªáu g·ªôp" sau n√≥
        brand_position = data_for_valid.columns.get_loc('Brand')
        data_for_valid.insert(brand_position + 1, 'Th∆∞∆°ng hi·ªáu g·ªôp', data_for_valid.pop('Th∆∞∆°ng hi·ªáu g·ªôp'))
        
        print("‚úÖ ƒê√£ s·∫Øp x·∫øp l·∫°i v·ªã tr√≠ c√°c c·ªôt")
        
        # 6. Hi·ªÉn th·ªã th·ªëng k√™
        print(f"\nüìä Th·ªëng k√™ sau khi c·∫≠p nh·∫≠t:")
        print(f"  - T·ªïng s·ªë b·∫£n ghi: {len(data_for_valid)}")
        print(f"  - C√¥ng ty nh·∫≠p g·ªôp ƒë∆∞·ª£c merge: {data_for_valid['C√¥ng ty nh·∫≠p g·ªôp'].notna().sum()}")
        print(f"  - Nh√† cung c·∫•p g·ªôp ƒë∆∞·ª£c merge: {data_for_valid['Nh√† Cung C·∫•p G·ªôp'].notna().sum()}")
        print(f"  - Th∆∞∆°ng hi·ªáu g·ªôp ƒë∆∞·ª£c merge: {data_for_valid['Th∆∞∆°ng hi·ªáu g·ªôp'].notna().sum()}")
        
        # Hi·ªÉn th·ªã top 5 c√¥ng ty nh·∫≠p g·ªôp
        if data_for_valid['C√¥ng ty nh·∫≠p g·ªôp'].notna().any():
            print(f"\nüìà Top 5 c√¥ng ty nh·∫≠p g·ªôp:")
            print(data_for_valid['C√¥ng ty nh·∫≠p g·ªôp'].value_counts().head())
        
    else:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file dim cty nh·∫≠p: {dim_file_path}")
        print("üîÑ T·∫°o c√°c c·ªôt tr·ªëng...")
        
        # T·∫°o c√°c c·ªôt tr·ªëng n·∫øu kh√¥ng t√¨m th·∫•y file dim
        data_for_valid['C√¥ng ty nh·∫≠p g·ªôp'] = None
        data_for_valid['Nh√† Cung C·∫•p G·ªôp'] = None
        data_for_valid['Th∆∞∆°ng hi·ªáu g·ªôp'] = None
        
        # S·∫Øp x·∫øp l·∫°i v·ªã tr√≠ c√°c c·ªôt
        ncc_position = data_for_valid.columns.get_loc('Nh√†_cung_c·∫•p')
        data_for_valid.insert(ncc_position + 1, 'Nh√† Cung C·∫•p G·ªôp', data_for_valid.pop('Nh√† Cung C·∫•p G·ªôp'))
        
        brand_position = data_for_valid.columns.get_loc('Brand')
        data_for_valid.insert(brand_position + 1, 'Th∆∞∆°ng hi·ªáu g·ªôp', data_for_valid.pop('Th∆∞∆°ng hi·ªáu g·ªôp'))

except Exception as e:
    print(f"‚ùå L·ªói khi x·ª≠ l√Ω file dim cty nh·∫≠p: {e}")
    print("üîÑ T·∫°o c√°c c·ªôt tr·ªëng...")
    
    # T·∫°o c√°c c·ªôt tr·ªëng n·∫øu c√≥ l·ªói
    data_for_valid['C√¥ng ty nh·∫≠p g·ªôp'] = None
    data_for_valid['Nh√† Cung C·∫•p G·ªôp'] = None
    data_for_valid['Th∆∞∆°ng hi·ªáu g·ªôp'] = None
    
    # S·∫Øp x·∫øp l·∫°i v·ªã tr√≠ c√°c c·ªôt
    ncc_position = data_for_valid.columns.get_loc('Nh√†_cung_c·∫•p')
    data_for_valid.insert(ncc_position + 1, 'Nh√† Cung C·∫•p G·ªôp', data_for_valid.pop('Nh√† Cung C·∫•p G·ªôp'))
    
    brand_position = data_for_valid.columns.get_loc('Brand')
    data_for_valid.insert(brand_position + 1, 'Th∆∞∆°ng hi·ªáu g·ªôp', data_for_valid.pop('Th∆∞∆°ng hi·ªáu g·ªôp'))

print("‚úÖ Ho√†n th√†nh c·∫≠p nh·∫≠t c√¥ng ty nh·∫≠p g·ªôp v√† th√™m c√°c c·ªôt m·ªõi!")







# Sort the merged DataFrame based on the original index
merged_df = data_for_valid.sort_index()
merged_df

# t·∫°o file m·ªõi v·ªõi c√°c c·ªôt ƒë√£ c·∫≠p nh·∫≠t
columns_to_select = [
    'Day', 'Month', 'Year', 'M√£_t·ªù_khai', 'C√¥ng_ty_nh·∫≠p', 'C√¥ng ty nh·∫≠p g·ªôp', 
    'C√¥ng_ty_nh·∫≠p (TA)', 'ƒê·ªãa_ch·ªâ', 'M√£_s·ªë_thu·∫ø', 'Nh√†_cung_c·∫•p', 'Nh√† Cung C·∫•p G·ªôp', 'ƒê·ªãa_ch·ªâ_(ncc)', 
    'Qu·ªëc_gia_xu·∫•t_x·ª©', 'HScode', 'M√¥_t·∫£_s·∫£n_ph·∫©m', 'Brand', 'Th∆∞∆°ng hi·ªáu g·ªôp', 'H·ª£p_l·ªá', 
    'Updated_S·ªë_l∆∞·ª£ng', 'Updated_ƒê∆°n_v·ªã', 'S·ªë_l∆∞·ª£ng', 'ƒê∆°n_v·ªã', 'Kh·ªëi_l∆∞·ª£ng', 
    'Th√†nh_ti·ªÅn', 'Ti·ªÅn_t·ªá', 'Updated_ƒê∆°n_gi√°', 'ƒê∆°n_gi√°', 'T·ª∑ gi√°', 
    'ƒêi·ªÅu_ki·ªán_giao_h√†ng', 'C·∫£ng xu·∫•t', 'C·∫£ng nh·∫≠p', 'MarketClassification', 
    'Ph√¢n lo·∫°i c√¥ng ty nh·∫≠p', 'is_duplicated', 'S·∫£n ph·∫©m'
]

new_column_names = [
    'Day', 'Month', 'Year', 'M√£ t·ªù khai', 'C√¥ng ty nh·∫≠p', 'C√¥ng ty nh·∫≠p g·ªôp', 
    'C√¥ng ty nh·∫≠p (TA)', 'ƒê·ªãa ch·ªâ', 'M√£ s·ªë thu·∫ø', 'Nh√† cung c·∫•p', 'Nh√† cung c·∫•p g·ªôp', 'ƒê·ªãa ch·ªâ (ncc)', 
    'Qu·ªëc gia xu·∫•t x·ª©', 'HScode', 'M√¥ t·∫£ s·∫£n ph·∫©m', 'Th∆∞∆°ng hi·ªáu', 'Th∆∞∆°ng hi·ªáu g·ªôp', 'H·ª£p l·ªá',    
    'updated S·ªë l∆∞·ª£ng', 'updated ƒê∆°n v·ªã', 'S·ªë l∆∞·ª£ng', 'ƒê∆°n v·ªã', 'Kh·ªëi l∆∞·ª£ng', 
    'Th√†nh ti·ªÅn', 'Ti·ªÅn t·ªá', 'Updated ƒê∆°n gi√°', 'ƒê∆°n gi√°', 'T·ª∑ gi√°', 
    'ƒêi·ªÅu ki·ªán giao h√†ng', 'C·∫£ng xu·∫•t', 'C·∫£ng nh·∫≠p', 'Ph√¢n lo·∫°i th·ªã tr∆∞·ªùng', 
    'Ph√¢n lo·∫°i c√¥ng ty nh·∫≠p', 'is duplicate', 'S·∫£n ph·∫©m'
]

# L·ªçc c√°c c·ªôt c√≥ s·∫µn trong dataframe
available_columns = [col for col in columns_to_select if col in merged_df.columns]
available_new_names = [new_column_names[i] for i, col in enumerate(columns_to_select) if col in merged_df.columns]

print(f"üìã C√°c c·ªôt s·∫Ω ƒë∆∞·ª£c xu·∫•t ({len(available_columns)} c·ªôt):")
for i, (old_name, new_name) in enumerate(zip(available_columns, available_new_names)):
    print(f"  {i+1:2d}. {old_name} ‚Üí {new_name}")

selected_columns_df = merged_df[available_columns]
selected_columns_df.columns = available_new_names

# Xu·∫•t file
output_path = f"../../Data/data final/Data Code Out/{newFileName}.xlsx"
selected_columns_df.to_excel(output_path, index=False, header=True, sheet_name="Data")

print(f"\n‚úÖ ƒê√£ xu·∫•t file th√†nh c√¥ng!")
print(f"üìÅ ƒê∆∞·ªùng d·∫´n: {output_path}")
print(f"üìä K√≠ch th∆∞·ªõc: {selected_columns_df.shape[0]} d√≤ng, {selected_columns_df.shape[1]} c·ªôt")




